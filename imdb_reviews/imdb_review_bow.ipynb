{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b6e65bb-7a3a-4215-9a6d-6d05d2f750d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yasht\\fakenews\\venv\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "\n",
    "import random\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecfe15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e54af23-12df-47e8-b5e4-ccf1a3b12a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_file = 'imdb_review_w2v.model' # change each time\n",
    "d2vdm_model_file = 'imdb_review_d2vdm.model' # change each time\n",
    "d2vdbow_model_file = 'imdb_review_d2vdbow.model' # change each time\n",
    "train_csv = 'train_df.csv'\n",
    "test_csv = 'test_df.csv'\n",
    "df_csv = 'df.csv'\n",
    "df_pkl = 'df.pkl'\n",
    "\n",
    "neg_bound = 4\n",
    "pos_bound = 7\n",
    "\n",
    "train_size = 0.80\n",
    "\n",
    "num_reviews = 10000\n",
    "doc_ids = np.asarray(range(num_reviews))\n",
    "\n",
    "min_occ = 5 # The minimum number of occurrences for a word to be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba3b705-8678-47db-b1f4-422cadff51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_vector(word):\n",
    "    \"\"\"Get the vector for a word\"\"\"\n",
    "    try:\n",
    "        return model.wv[word]\n",
    "    except:\n",
    "        print(word)\n",
    "        raise\n",
    "        \n",
    "def filter_tokens(tokens, vocab=None):\n",
    "    if vocab is None:\n",
    "        vocab = w2v_vocab\n",
    "    return [token for token in tokens if token in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40aa70d-76bf-4788-9356-c8f2a1df7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    # Remove non-word characters\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "    # Remove single letters\n",
    "    text = re.sub(r'\\b[a-z]{0,3}\\b', ' ', text)\n",
    "    # Merge multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = text.split()\n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a129d4-b526-49a6-8ff7-d28bc6da6306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_stratified(X, labels):\n",
    "    return train_test_split(X, stratify=labels, train_size=train_size, random_state=1)\n",
    "\n",
    "def my_train_test_split(X):\n",
    "    return split_stratified(X, df['Rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd27bbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7595b71-d97b-4a82-854f-ceae46ad7f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "half_reviews = num_reviews // 2\n",
    "assert half_reviews == num_reviews / 2\n",
    "\n",
    "def load_train_or_test(dir):\n",
    "    random.seed(3)\n",
    "    \"\"\"\n",
    "    Return the negative and positive train or test data\n",
    "    \"\"\"\n",
    "    def load_neg_or_pos(sub, is_pos):\n",
    "        res = []\n",
    "        for file_name in os.listdir(sub):\n",
    "            with open(sub + file_name, encoding='utf8') as file:\n",
    "                underscore_ind = file_name.index('_')\n",
    "                period_ind = file_name.index('.')\n",
    "                id = int(file_name[:underscore_ind])\n",
    "                rating = int(file_name[underscore_ind + 1:period_ind])\n",
    "                text = next(file)\n",
    "                res.append([rating, text])\n",
    "        return res\n",
    "    # Only choose more polar ratings\n",
    "    neg = [[rating, text] for rating, text in load_neg_or_pos(dir + '/neg/', False) if rating <= neg_bound]\n",
    "    pos = [[rating, text] for rating, text in load_neg_or_pos(dir + '/pos/', True) if rating >= pos_bound]\n",
    "    random.shuffle(neg)\n",
    "    random.shuffle(pos)\n",
    "    both = neg[:half_reviews] + pos[:half_reviews]\n",
    "    random.shuffle(both)\n",
    "    return pd.DataFrame(both, columns=['Rating', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab31fc82-cf15-49a8-a3a2-ddcf787f3cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_train_or_test('./train') #.append(load_train_or_test('./test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a53694d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>this may not be War &amp; Peace, but the two Acade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>after seeing this film for the 3rd time now i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>When 'My Deja Vu, My Deja Vu' aired last seaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>\"Magic\" isn't too strong a word for the spell ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>You could stage a version of Charles Dickens' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>8</td>\n",
       "      <td>Judy Davis shows us here why she is one of Aus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>2</td>\n",
       "      <td>Don't get me wrong, I love action and revenge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>8</td>\n",
       "      <td>Im a huge M Lillard fan that's why I ended up ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>8</td>\n",
       "      <td>In Iran women are prohibited from attending li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4</td>\n",
       "      <td>The movie had a cute opening, I truly believed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rating                                               Text\n",
       "0         10  this may not be War & Peace, but the two Acade...\n",
       "1         10  after seeing this film for the 3rd time now i ...\n",
       "2          1  When 'My Deja Vu, My Deja Vu' aired last seaso...\n",
       "3         10  \"Magic\" isn't too strong a word for the spell ...\n",
       "4         10  You could stage a version of Charles Dickens' ...\n",
       "...      ...                                                ...\n",
       "9995       8  Judy Davis shows us here why she is one of Aus...\n",
       "9996       2  Don't get me wrong, I love action and revenge ...\n",
       "9997       8  Im a huge M Lillard fan that's why I ended up ...\n",
       "9998       8  In Iran women are prohibited from attending li...\n",
       "9999       4  The movie had a cute opening, I truly believed...\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe27c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokens'] = df['Text'].apply(tokenize)\n",
    "# Clean up the text too\n",
    "df['Text'] = df['Tokens'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "741a2760-65b6-404e-98b9-63a1abe00252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>peace academy noms forthcoming genius james wo...</td>\n",
       "      <td>[peace, academy, noms, forthcoming, genius, ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>seeing film time think almost adam worst film ...</td>\n",
       "      <td>[seeing, film, time, think, almost, adam, wors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>deja deja aired last season pleased scrub thou...</td>\n",
       "      <td>[deja, deja, aired, last, season, pleased, scr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>magic strong word spell film weave find relaxi...</td>\n",
       "      <td>[magic, strong, word, spell, film, weave, find...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>could stage version charles dickens christmas ...</td>\n",
       "      <td>[could, stage, version, charles, dickens, chri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>8</td>\n",
       "      <td>judy davis show australia respected loved acto...</td>\n",
       "      <td>[judy, davis, show, australia, respected, love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>2</td>\n",
       "      <td>wrong love action revenge flick seen many sinc...</td>\n",
       "      <td>[wrong, love, action, revenge, flick, seen, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>8</td>\n",
       "      <td>huge lillard ended watching movie honestly dou...</td>\n",
       "      <td>[huge, lillard, ended, watching, movie, honest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>8</td>\n",
       "      <td>iran woman prohibited attending live sporting ...</td>\n",
       "      <td>[iran, woman, prohibited, attending, live, spo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>4</td>\n",
       "      <td>movie cute opening truly believed best romanti...</td>\n",
       "      <td>[movie, cute, opening, truly, believed, best, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rating                                               Text  \\\n",
       "0         10  peace academy noms forthcoming genius james wo...   \n",
       "1         10  seeing film time think almost adam worst film ...   \n",
       "2          1  deja deja aired last season pleased scrub thou...   \n",
       "3         10  magic strong word spell film weave find relaxi...   \n",
       "4         10  could stage version charles dickens christmas ...   \n",
       "...      ...                                                ...   \n",
       "9995       8  judy davis show australia respected loved acto...   \n",
       "9996       2  wrong love action revenge flick seen many sinc...   \n",
       "9997       8  huge lillard ended watching movie honestly dou...   \n",
       "9998       8  iran woman prohibited attending live sporting ...   \n",
       "9999       4  movie cute opening truly believed best romanti...   \n",
       "\n",
       "                                                 Tokens  \n",
       "0     [peace, academy, noms, forthcoming, genius, ja...  \n",
       "1     [seeing, film, time, think, almost, adam, wors...  \n",
       "2     [deja, deja, aired, last, season, pleased, scr...  \n",
       "3     [magic, strong, word, spell, film, weave, find...  \n",
       "4     [could, stage, version, charles, dickens, chri...  \n",
       "...                                                 ...  \n",
       "9995  [judy, davis, show, australia, respected, love...  \n",
       "9996  [wrong, love, action, revenge, flick, seen, ma...  \n",
       "9997  [huge, lillard, ended, watching, movie, honest...  \n",
       "9998  [iran, woman, prohibited, attending, live, spo...  \n",
       "9999  [movie, cute, opening, truly, believed, best, ...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a95475f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "# df.to_csv(df_csv)\n",
    "df.to_pickle(df_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7fa98e33-e3b9-4b96-b405-83a6d7dcbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train and save model\n",
    "train_tokens, test_tokens = my_train_test_split(df['Tokens'])\n",
    "w2v_model = Word2Vec(sentences=train_tokens)\n",
    "w2v_model.save(w2v_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d445870",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab = set(w2v_model.wv.key_to_index.keys())\n",
    "w2v_vocab_ord = np.array(list(w2v_model.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab274690-2bae-4dba-8339-7e700c626e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep only tokens that showed up the required number of times\n",
    "# train_df['Tokens'] = train_df['Tokens'].apply(filter_tokens)\n",
    "\n",
    "# test_df['Tokens'] = test_df['Text'].apply(lambda text: filter_tokens(tokenize(text)))\n",
    "# # Process test text too\n",
    "# test_df['Text'] = test_df['Tokens'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7e7bff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The vectors corresponding to each reviews' words\n",
    "# df['Vectors'] = df['Tokens'].apply(get_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61c9109d-6813-4d39-91ff-4244017239d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train Doc2Vec model\n",
    "import collections\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "tagged_docs = [TaggedDocument(words=df['Tokens'][doc_id], tags=[doc_id]) for doc_id in doc_ids]\n",
    "assert type(tagged_docs[0].words) == list\n",
    "# print(len(tagged_docs), type(tagged_docs[0].words), tagged_docs[0])\n",
    "X_train_docs, X_test_docs = my_train_test_split(tagged_docs)\n",
    "\n",
    "class PrintLoss(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def on_epoch_begin(self, model):\n",
    "        model.running_training_loss = 0.0\n",
    "#         print(f'Starting epoch {self.epoch}', end=' | ')\n",
    "    \n",
    "    def on_epoch_end(self, model):\n",
    "#         print(f'Finished epoch {self.epoch}, loss = {model.get_latest_training_loss()}')\n",
    "        self.epoch += 1\n",
    "\n",
    "def train_d2v(model):\n",
    "    model.random.seed(1)\n",
    "    model.build_vocab(X_train_docs)\n",
    "    model.train(X_train_docs, total_examples=model.corpus_count, epochs=model.epochs, compute_loss=True, callbacks=[PrintLoss()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f98bf014-cbd4-4743-8df7-ad2b70fd8df9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained d2vdm\n",
      "Trained d2vdbow\n"
     ]
    }
   ],
   "source": [
    "d2vdm_model = train_d2v(Doc2Vec(dm=1, vector_size=50, min_count=min_occ, epochs=100, seed=1))\n",
    "d2vdm_model.save(d2vdm_model_file)\n",
    "print('Trained d2vdm')\n",
    "\n",
    "d2vdbow_model = train_d2v(Doc2Vec(dm=0, vector_size=50, min_count=min_occ, epochs=100, seed=1))\n",
    "d2vdbow_model.save(d2vdbow_model_file)\n",
    "print('Trained d2vdbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cd9d051c-6e85-4d1a-9914-2731ee86b592",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained d2vdm\n",
      "Trained d2vdbow\n"
     ]
    }
   ],
   "source": [
    "d2vdm100_model = train_d2v(Doc2Vec(dm=1, vector_size=100, min_count=min_occ, epochs=100, seed=1))\n",
    "d2vdm100_model.save('d2vdm100.model')\n",
    "print('Trained d2vdm100')\n",
    "\n",
    "d2vdbow100_model = train_d2v(Doc2Vec(dm=0, vector_size=100, min_count=min_occ, epochs=100, seed=1))\n",
    "d2vdbow100_model.save('d2vdbow100.model')\n",
    "print('Trained d2vdbow100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4f349b1-6e67-403b-ac42-cfc079831e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_tagged_docs = [TaggedDocument(words=df['Tokens'][doc_id], tags=[y_bi[doc_id]]) for doc_id in doc_ids]\n",
    "X_train_docs, X_test_docs = my_train_test_split(direct_tagged_docs)\n",
    "\n",
    "def train_d2v_direct(model):\n",
    "    model.random.seed(1)\n",
    "    model.build_vocab(X_train_docs)\n",
    "    model.train(X_train_docs, total_examples=model.corpus_count, epochs=model.epochs, compute_loss=True, callbacks=[PrintLoss()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68da8cb1-8c18-4c09-9dda-7d0c7b3f37b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained d2vdmdirect\n",
      "Trained d2vdbowdirect\n"
     ]
    }
   ],
   "source": [
    "d2vdmdirect_model = train_d2v_direct(Doc2Vec(dm=1, vector_size=50, min_count=min_occ, epochs=100, seed=1))\n",
    "d2vdmdirect_model.save('d2vdmdirect.model')\n",
    "print('Trained d2vdmdirect')\n",
    "\n",
    "d2vdbowdirect_model = train_d2v_direct(Doc2Vec(dm=1, vector_size=50, min_count=min_occ, epochs=100, seed=1))\n",
    "d2vdbowdirect_model.save('d2vdbowdirect.model')\n",
    "print('Trained d2vdbowdirect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ca1f755a-5b99-4fbb-86b5-85d491b81dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_d2v_sim(model, doc1, doc2):\n",
    "    \"\"\"Get cos similarity of 2 docs' vectors\"\"\"\n",
    "    vector1 = model.infer_vector(doc1)\n",
    "    vector2 = model.infer_vector(doc2)\n",
    "    return cosine_similarity([vector1], [vector2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14f63b74-c86c-4f0a-b8bf-a01004d1341a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>9</td>\n",
       "      <td>bromwell high cartoon comedy time program scho...</td>\n",
       "      <td>[bromwell, high, cartoon, comedy, time, progra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>9</td>\n",
       "      <td>bromwell high nothing short brilliant expertly...</td>\n",
       "      <td>[bromwell, high, nothing, short, brilliant, ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rating                                               Text  \\\n",
       "55         9  bromwell high cartoon comedy time program scho...   \n",
       "1298       9  bromwell high nothing short brilliant expertly...   \n",
       "\n",
       "                                                 Tokens  \n",
       "55    [bromwell, high, cartoon, comedy, time, progra...  \n",
       "1298  [bromwell, high, nothing, short, brilliant, ex...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_reviews = df[df['Tokens'].apply(lambda x: 'bromwell' in x)]\n",
    "sim_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46c0a49b-8fa7-4760-9167-09f18bdffd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62547696]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1, r2 = sim_reviews.Tokens\n",
    "get_d2v_sim(d2vdm_model, r1, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e52e5f25-8216-4e62-96a4-117df37823b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44448546]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_d2v_sim(d2vdbow_model, r1, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5c974-c54a-4b3c-ac96-4c89f4f6718b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s1.min(), s1.max(), s1.mean(), np.median(s1), s1.min(), s2.max(), s2.mean(), np.median(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c63f1",
   "metadata": {},
   "source": [
    "# Load stuff done already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02492993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(df_csv)\n",
    "df = pd.read_pickle(df_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ae87b0b-22bc-48c6-a1be-82d12345457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained Word2Vec model\n",
    "w2v_model = Word2Vec.load(w2v_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91fad42f-4028-4a74-b177-196172feb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained Doc2Vec models\n",
    "d2vdm_model = Doc2Vec.load(d2vdm_model_file)\n",
    "d2vdbow_model = Doc2Vec.load(d2vdbow_model_file)\n",
    "d2vdm100_model = Doc2Vec.load('d2vdm100.model')\n",
    "d2vdbow100_model = Doc2Vec.load('d2vdbow100.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b70547-0a53-4091-8a2e-9adb64d85a42",
   "metadata": {},
   "source": [
    "# Common stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "408dfb54-25c0-4ddf-a1bd-0979f7a3cd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab = set(w2v_model.wv.key_to_index.keys())\n",
    "w2v_vocab_ord = np.array(list(w2v_model.wv.key_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "472d45b3-ece5-4681-8e70-328a7f428e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_bin2 = df['Rating'] // 2\n",
    "# y_train_bin2, y_test_bin2 = split_stratified(y_bin2, y_bin2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b4fdd-a4e5-4a17-bfd9-c1960f0597f3",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f3b0d7e-edc3-4130-aefb-ac39c4adb06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bi = df['Rating'] > 5\n",
    "y_train_bi, y_test_bi = my_train_test_split(y_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dae55314-785f-4a22-b019-828b2c2d0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_results = []\n",
    "\n",
    "def score_classifier(classifier, X_test, y_test=y_test_bi, big_table=True, name='SOME MODEL'):\n",
    "    score = classifier.score(X_test, y_test)\n",
    "    predicted = classifier.predict(X_test)\n",
    "    cm = confusion_matrix(predicted, y_test)\n",
    "    report = classification_report(predicted, y_test)\n",
    "    \n",
    "    classifier_results.append([name, classifier, score, cm])\n",
    "    \n",
    "    print_results(score, cm, report, big_table)\n",
    "    \n",
    "def print_results(score, cm, class_report, big_table=True):\n",
    "    print('Mean accuracy:', score)\n",
    "    print(f\"TP: {cm[0][0]}, FN: {cm[0][1]}\\nFP: {cm[1][0]}, TN: {cm[1][1]}\")\n",
    "    if big_table:\n",
    "        print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53add2ff-ee1b-4fc1-9caf-60a1820a4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_classifier(classifier, X_train, X_test, big_table=True, name='SOME MODEL'):\n",
    "    classifier.fit(X_train, y_train_bi)\n",
    "    score_classifier(classifier, X_test, big_table=big_table, name=name)\n",
    "\n",
    "def try_tfidf_classifier(classifier, big_table=True):\n",
    "    try_classifier(classifier, X_train_tfidf, X_test_tfidf, big_table=big_table, name='tf-idf')\n",
    "\n",
    "def try_d2vdm_classifier(classifier, big_table=True):\n",
    "    try_classifier(classifier, X_train_d2vdm, X_test_d2vdm, big_table=big_table, name='PV-DM')\n",
    "    \n",
    "def try_d2vdbow_classifier(classifier, big_table=True):\n",
    "    try_classifier(classifier, X_train_d2vdbow, X_test_d2vdbow, big_table=big_table, name='PV-DBOW')\n",
    "    \n",
    "def try_d2vdm100_classifier(classifier, big_table=True):\n",
    "    try_classifier(classifier, X_train_d2vdm100, X_test_d2vdm100, big_table=big_table, name='PV-DM (100 dims)')\n",
    "    \n",
    "def try_d2vdbow100_classifier(classifier, big_table=True):\n",
    "    try_classifier(classifier, X_train_d2vdbow100, X_test_d2vdbow100, big_table=big_table, name='PV-DBOW (100 dims)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4eda3f3-34c3-4154-843b-7bf4020f85e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 14577) 14577\n",
      "(8000, 33867) 33867\n",
      "(8000, 19290) 19290\n"
     ]
    }
   ],
   "source": [
    "# Make different train-test splits for tf-idf\n",
    "def make_tfidf(**kwargs):\n",
    "    # en_stop because the default apparently has problems\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=en_stop, min_df=min_occ, **kwargs)\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df['Text'])\n",
    "    X_train_tfidf, X_test_tfidf = my_train_test_split(X_tfidf)\n",
    "    \n",
    "    print(X_train_tfidf.shape, len(tfidf_vectorizer.vocabulary_.keys()))\n",
    "    \n",
    "    return X_train_tfidf, X_test_tfidf\n",
    "\n",
    "X_train_tfidf_1, X_test_tfidf_1 = make_tfidf(ngram_range=(1, 1))\n",
    "X_train_tfidf_1_2, X_test_tfidf_1_2 = make_tfidf(ngram_range=(1, 2))\n",
    "X_train_tfidf_2, X_test_tfidf_2 = make_tfidf(ngram_range=(2, 2))\n",
    "\n",
    "# The \"best\" tf-idf model\n",
    "X_train_tfidf = X_train_tfidf_1_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff6891bb-93a7-4f1f-999c-a5a89db76c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make different train-test splits for Doc2Vec\n",
    "def split_d2v(model):\n",
    "    vectors = df['Tokens'].apply(model.infer_vector)\n",
    "    train, test = my_train_test_split(vectors)\n",
    "    return np.stack(train), np.stack(test)\n",
    "\n",
    "X_train_d2vdm, X_test_d2vdm = split_d2v(d2vdm_model)\n",
    "X_train_d2vdbow, X_test_d2vdbow = split_d2v(d2vdbow_model)\n",
    "X_train_d2vdm100, X_test_d2vdm100 = split_d2v(d2vdm100_model)\n",
    "X_train_d2vdbow100, X_test_d2vdbow100 = split_d2v(d2vdbow100_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db7c8611-5014-4d8e-8331-81937952c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make different train-test splits for Doc2Vec\n",
    "        \n",
    "def make_w2v(model):\n",
    "    \"\"\"Average vectors in doc, then train-test split\"\"\"\n",
    "    \n",
    "    def average_doc_w2v_vectors(doc):\n",
    "        \"\"\"Average Word2Vec vectors of document\"\"\"\n",
    "        sum_vec = None\n",
    "        num = 0\n",
    "        for word in doc:\n",
    "            if sum_vec is None:\n",
    "                sum_vec = np.zeros(model.vector_size)\n",
    "                num += 1\n",
    "            elif word in model.wv.key_to_index:\n",
    "                sum_vec += model.wv[word]\n",
    "                num += 1\n",
    "        if num:\n",
    "            return sum_vec / num\n",
    "        else:\n",
    "            raise\n",
    "#             return np.zeros(w2v_model.vector_size)\n",
    "    \n",
    "    averaged = df['Tokens'].apply(average_doc_w2v_vectors)\n",
    "    \n",
    "    return my_train_test_split(np.stack(averaged))\n",
    "\n",
    "X_train_w2vavg, X_test_w2vavg = make_w2v(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac61b40-6941-4cb0-87bc-a4ac59211656",
   "metadata": {
    "tags": []
   },
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a0609efb-4b5e-4a0c-80a5-082308cb6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "sentiment_cmap = matplotlib.colors.ListedColormap(['red', 'blue'])\n",
    "\n",
    "def try_d2v_tsne(X_tsne, size_2d=(15, 15)):\n",
    "#     cm = plt.cm.get_cmap('gist_rainbow')\n",
    "    font_size = 'x-large'\n",
    "    \n",
    "    transposed = X_tsne.transpose()\n",
    "    xs, ys = transposed\n",
    "    \n",
    "    fig = plt.figure(figsize=size_2d)\n",
    "    ax = plt.axes()\n",
    "    # Plot each word, with the third feature being \n",
    "    plot = plt.scatter(xs, ys, c=y_train_bi, cmap=sentiment_cmap)\n",
    "    \n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    \n",
    "#     ax.set_xlim(xlim[0], xlim[1])\n",
    "#     ax.set_ylim(ylim[0], ylim[1])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def try_d2v_tsne_3d(X_tsne, size_3d=(15, 15), xlim=(-150, 300), ylim=(-600, 250), zlim=(-150, 300)):\n",
    "    font_size = 'x-large'\n",
    "    \n",
    "    transposed = X_tsne.transpose()\n",
    "    xs, ys, zs = transposed\n",
    "    neg_xs = []\n",
    "    neg_ys = []\n",
    "    neg_zs = []\n",
    "    pos_xs = []\n",
    "    pos_ys = []\n",
    "    pos_zs = []\n",
    "    \n",
    "    for x, y, z, res in zip(xs, ys, zs, y_train_bi):\n",
    "        _xs, _ys, _zs = [pos_xs, pos_ys, pos_zs] if res else [neg_xs, neg_ys, neg_zs]\n",
    "        _xs.append(x)\n",
    "        _ys.append(y)\n",
    "        _zs.append(z)\n",
    "    \n",
    "    fig = plt.figure(figsize=size_3d)\n",
    "    ax = plt.axes(projection='3d')\n",
    "    # Red is for negative reviews, blue is for positive ones\n",
    "    plot = ax.scatter(xs, ys, zs, c=y_train_bi, cmap=sentiment_cmap)\n",
    "    \n",
    "#     plt.colorbar(plot)\n",
    "\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_zlabel('Feature 3')\n",
    "    \n",
    "    ax.set_xlim(xlim[0], xlim[1])\n",
    "    ax.set_ylim(ylim[0], ylim[1])\n",
    "    ax.set_zlim(zlim[0], zlim[1])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Just plot negative reviews\n",
    "    fig = plt.figure(figsize=size_3d)\n",
    "    ax = plt.axes(projection='3d')\n",
    "    plot = ax.scatter(neg_xs, neg_ys, neg_zs, c='red')\n",
    "    \n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_zlabel('Feature 3')\n",
    "    \n",
    "    ax.set_xlim(xlim[0], xlim[1])\n",
    "    ax.set_ylim(ylim[0], ylim[1])\n",
    "    ax.set_zlim(zlim[0], zlim[1])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Just plot positive reviews\n",
    "    fig = plt.figure(figsize=size_3d)\n",
    "    ax = plt.axes(projection='3d')\n",
    "    plot = ax.scatter(pos_xs, pos_ys, pos_zs, c='blue')\n",
    "\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_zlabel('Feature 3')\n",
    "    \n",
    "    ax.set_xlim(xlim[0], xlim[1])\n",
    "    ax.set_ylim(ylim[0], ylim[1])\n",
    "    ax.set_zlim(zlim[0], zlim[1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "118c689e-40bf-43c2-b533-56d5239b303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne_d2vdm2d = TSNE(n_components=2).fit_transform(X_train_d2vdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "887c4ae8-de62-40c8-b2ea-3f1863185761",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne_d2vdm3d = TSNE(n_components=3).fit_transform(X_train_d2vdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3f981479-31c5-4e78-821b-d3c946d3b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne_d2vdm3d100 = TSNE(n_components=3).fit_transform(X_train_d2vdm100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "98d280d1-1abd-41e4-a80b-5353905123d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne_d2vdbow2d = TSNE(n_components=2).fit_transform(X_train_d2vdbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef26c8-2d1e-4e43-abb4-0b7f89a5e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne_d2vdbow3d = TSNE(n_components=3).fit_transform(X_train_d2vdbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd62b4-445b-41ac-a105-c147a1859152",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne_d2vdbow3d100 = TSNE(n_components=3).fit_transform(X_train_d2vdbow100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e19f84-d8b5-42a0-848e-9055e7c0e61e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Current boundaries miss outliers!\n",
    "try_d2v_tsne_3d(X_tsne_d2vdm3d100, xlim=(-150, 0), ylim=(75, 250), zlim=(-150, -40), size_3d=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55318730-cad0-4683-baf0-b56d81eb6ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Current boundaries miss outliers!\n",
    "try_d2v_tsne_3d(X_tsne_d2vdbow3d100, xlim=(-150, 0), ylim=(75, 250), zlim=(-150, -40), size_3d=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd42d1-e707-46e1-a8e6-02f933220014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Current boundaries miss outliers!\n",
    "try_d2v_tsne_3d(X_tsne_d2vdm3d, xlim=(-150, 0), ylim=(75, 250), zlim=(-150, -40), size_3d=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991ebe0-827e-42b3-b075-5e045feb3a04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Current boundaries miss outliers!\n",
    "try_d2v_tsne_3d(X_tsne_d2vdbow3d, xlim=(-150, 0), ylim=(75, 250), zlim=(-150, -20), size_3d=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb9072",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logistic Regression + Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f4a46df-3bb7-4c34-af14-4479d1eb575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_bow(**kwargs):\n",
    "    cnt_vectorizer = CountVectorizer(stop_words=en_stop, **kwargs) # en_stop because the default has problems\n",
    "    X_bow = cnt_vectorizer.fit_transform(df['Text'])\n",
    "    X_train_bow, X_test_bow = my_train_test_split(X_bow)\n",
    "\n",
    "    # Scale data\n",
    "    scaler_bow = StandardScaler(with_mean=False).fit(X_train_bow)\n",
    "    X_train_bow_scaled = scaler_bow.transform(X_train_bow)\n",
    "    X_test_bow_scaled = scaler_bow.transform(X_test_bow)\n",
    "#     print(X_train_bow_scaled.shape, len(cnt_vectorizer.vocabulary_.keys()))\n",
    "    \n",
    "    lr_bow = LogisticRegression(max_iter=2000)\n",
    "    lr_bow.fit(X_train_bow_scaled, y_train_bi)\n",
    "    \n",
    "    score_classifier(lr_bow, X_test_bow_scaled, big_table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2408e0fc-febf-493c-b9c1-34071d65127f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8305\n",
      "TP: 826, FN: 165\n",
      "FP: 174, TN: 835\n"
     ]
    }
   ],
   "source": [
    "try_bow(min_df=5, ngram_range=(1, 1)) # Just unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1072c298-f3ce-4087-a40e-493530dc786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.848\n",
      "TP: 853, FN: 157\n",
      "FP: 147, TN: 843\n"
     ]
    }
   ],
   "source": [
    "try_bow(min_df=5, ngram_range=(1, 2)) # Unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bd5af37-5983-433a-b714-b00b1dbd8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.751\n",
      "TP: 751, FN: 249\n",
      "FP: 249, TN: 751\n"
     ]
    }
   ],
   "source": [
    "try_bow(min_df=5, ngram_range=(2, 2)) # Just bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27d89c95-8810-4ffb-98cd-b7d6b96d4341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.845\n",
      "TP: 846, FN: 156\n",
      "FP: 154, TN: 844\n"
     ]
    }
   ],
   "source": [
    "try_bow(min_df=5, ngram_range=(1, 3)) # Unigrams, bigrams, and trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb06521-27c6-4752-b371-2b83b1ea49fe",
   "metadata": {},
   "source": [
    "## Logistic Regression + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d72a3ee9-3459-42f7-8e15-5f1c59e7ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_tfidf(X_train_tfidf, X_test_tfidf):    \n",
    "    lr_tfidf = LogisticRegression()\n",
    "    lr_tfidf.fit(X_train_tfidf, y_train_bi)\n",
    "    \n",
    "    score_classifier(lr_tfidf, X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0bb2ad3-7d79-472c-9a80-64a33431baf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 14577) 14577\n",
      "(8000, 33867) 33867\n",
      "(8000, 19290) 19290\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf_1, X_test_tfidf_1 = make_tfidf(ngram_range=(1, 1))\n",
    "X_train_tfidf_1_2, X_test_tfidf_1_2 = make_tfidf(ngram_range=(1, 2))\n",
    "X_train_tfidf_2, X_test_tfidf_2 = make_tfidf(ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d87f23a-21ae-4805-ad27-e58cb459ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.862\n",
      "TP: 846, FN: 122\n",
      "FP: 154, TN: 878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.87      0.86       968\n",
      "        True       0.88      0.85      0.86      1032\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.86      0.86      0.86      2000\n",
      "weighted avg       0.86      0.86      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_tfidf(X_train_tfidf_1, X_test_tfidf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb1c63e4-7843-4a89-883d-9993d5703ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.866\n",
      "TP: 855, FN: 123\n",
      "FP: 145, TN: 877\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.87      0.86       978\n",
      "        True       0.88      0.86      0.87      1022\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.87      0.87      0.87      2000\n",
      "weighted avg       0.87      0.87      0.87      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_tfidf(X_train_tfidf_1_2, X_test_tfidf_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8411dfc5-bbf8-49c7-befb-6f619fff25cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.799\n",
      "TP: 771, FN: 173\n",
      "FP: 229, TN: 827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.82      0.79       944\n",
      "        True       0.83      0.78      0.80      1056\n",
      "\n",
      "    accuracy                           0.80      2000\n",
      "   macro avg       0.80      0.80      0.80      2000\n",
      "weighted avg       0.80      0.80      0.80      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_tfidf(X_train_tfidf_2, X_test_tfidf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "767f3030-1ea9-40c3-b9a8-a1d84f8d3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the best tf-idf model\n",
    "X_train_tfidf = X_train_tfidf_1_2\n",
    "X_test_tfidf = X_test_tfidf_1_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19adc0e-6c5b-4136-9fae-2154333bbc18",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic Regression + Doc2Vec (DM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f50e2d3e-91b4-431b-8f9f-66e5bd713059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.848\n",
      "TP: 888, FN: 192\n",
      "FP: 112, TN: 808\n"
     ]
    }
   ],
   "source": [
    "score_classifier(LogisticRegression().fit(X_train_d2vdm, y_train_bi), X_test_d2vdm, big_table=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c3bdd1-0935-4ce2-baca-2388b9b9ed56",
   "metadata": {},
   "source": [
    "## Logistic Regression + Doc2Vec (DBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c35915c8-24d3-4173-a399-980ad35a9b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8615\n",
      "TP: 872, FN: 149\n",
      "FP: 128, TN: 851\n"
     ]
    }
   ],
   "source": [
    "score_classifier(LogisticRegression().fit(X_train_d2vdbow, y_train_bi), X_test_d2vdbow, big_table=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bbef6d-49f1-4fbf-83fb-c5ca6f0b19c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic Regression + Doc2Vec (DM, 100 dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6de7842-89e7-4248-9562-9c992738c3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8445\n",
      "TP: 880, FN: 191\n",
      "FP: 120, TN: 809\n"
     ]
    }
   ],
   "source": [
    "score_classifier(LogisticRegression().fit(X_train_d2vdm100, y_train_bi), X_test_d2vdm100, big_table=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9cd738-07a8-4609-8ae9-ebde17001f1d",
   "metadata": {},
   "source": [
    "## Logistic Regression + Doc2Vec (DBOW, 100 dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5599eb8-bf98-496b-8560-cb277f7fc997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8585\n",
      "TP: 865, FN: 148\n",
      "FP: 135, TN: 852\n"
     ]
    }
   ],
   "source": [
    "score_classifier(LogisticRegression().fit(X_train_d2vdbow100, y_train_bi), X_test_d2vdbow100, big_table=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2328234-449d-4083-b545-5411c451b82a",
   "metadata": {},
   "source": [
    "## Logistic Regression + Word2Vec (avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdee0e20-2491-4c10-b1d4-22607b409e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.753\n",
      "TP: 760, FN: 254\n",
      "FP: 240, TN: 746\n"
     ]
    }
   ],
   "source": [
    "score_classifier(LogisticRegression().fit(X_train_w2vavg, y_train_bi), X_test_w2vavg, big_table=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1047c-5dc6-4d2f-9b80-b64d69945e76",
   "metadata": {},
   "source": [
    "# Random forests + tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "096ea0d9-3fbd-4548-9be8-7936f17576bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def try_random_forests(model_name):\n",
    "    # TODO this is horrible\n",
    "    X_train = eval('X_train_' + model_name)\n",
    "    X_test = eval('X_test_' + model_name)\n",
    "    random_forest_classifiers = [\n",
    "        RandomForestClassifier(max_depth=5, random_state=1),\n",
    "        RandomForestClassifier(max_depth=5, min_samples_leaf=5, random_state=1),\n",
    "        RandomForestClassifier(max_depth=7, min_samples_leaf=5, n_estimators=200, random_state=1),\n",
    "        RandomForestClassifier(max_depth=9, min_samples_leaf=5, n_estimators=200, random_state=1),\n",
    "        RandomForestClassifier(max_depth=15, min_samples_leaf=5, n_estimators=300, random_state=1),\n",
    "        RandomForestClassifier(max_depth=15, min_samples_leaf=10, n_estimators=300, random_state=2),\n",
    "        RandomForestClassifier(max_depth=15, min_samples_leaf=15, n_estimators=300, random_state=2),\n",
    "        RandomForestClassifier(max_depth=16, n_estimators=400, random_state=2),\n",
    "        RandomForestClassifier(max_depth=20, n_estimators=600, random_state=1),\n",
    "        RandomForestClassifier(max_depth=20, n_estimators=800, random_state=1),\n",
    "    ]\n",
    "    for rfc in random_forest_classifiers:\n",
    "        print(rfc)\n",
    "        try_classifier(rfc, X_train, X_test, big_table=False, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "631807b5-9901-4843-9cab-115a290a1153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=5, random_state=1)\n",
      "Mean accuracy: 0.8095\n",
      "TP: 773, FN: 154\n",
      "FP: 227, TN: 846\n",
      "RandomForestClassifier(max_depth=5, min_samples_leaf=5, random_state=1)\n",
      "Mean accuracy: 0.816\n",
      "TP: 775, FN: 143\n",
      "FP: 225, TN: 857\n",
      "RandomForestClassifier(max_depth=7, min_samples_leaf=5, n_estimators=200,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.828\n",
      "TP: 782, FN: 126\n",
      "FP: 218, TN: 874\n",
      "RandomForestClassifier(max_depth=9, min_samples_leaf=5, n_estimators=200,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.8335\n",
      "TP: 795, FN: 128\n",
      "FP: 205, TN: 872\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=5, n_estimators=300,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.837\n",
      "TP: 801, FN: 127\n",
      "FP: 199, TN: 873\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=10, n_estimators=300,\n",
      "                       random_state=2)\n",
      "Mean accuracy: 0.83\n",
      "TP: 791, FN: 131\n",
      "FP: 209, TN: 869\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=15, n_estimators=300,\n",
      "                       random_state=2)\n",
      "Mean accuracy: 0.8335\n",
      "TP: 792, FN: 125\n",
      "FP: 208, TN: 875\n",
      "RandomForestClassifier(max_depth=16, n_estimators=400, random_state=2)\n",
      "Mean accuracy: 0.846\n",
      "TP: 818, FN: 126\n",
      "FP: 182, TN: 874\n",
      "RandomForestClassifier(max_depth=20, n_estimators=600, random_state=1)\n",
      "Mean accuracy: 0.849\n",
      "TP: 829, FN: 131\n",
      "FP: 171, TN: 869\n",
      "RandomForestClassifier(max_depth=20, n_estimators=800, random_state=1)\n",
      "Mean accuracy: 0.8495\n",
      "TP: 831, FN: 132\n",
      "FP: 169, TN: 868\n"
     ]
    }
   ],
   "source": [
    "try_random_forests('tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c244f4-af99-477c-8e5a-c49d3d6a0cf9",
   "metadata": {},
   "source": [
    "## Random Forest + Doc2Vec (DM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9faa96dd-db56-4f6d-9c49-205a3b822cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=5, random_state=1)\n",
      "Mean accuracy: 0.794\n",
      "TP: 808, FN: 220\n",
      "FP: 192, TN: 780\n",
      "RandomForestClassifier(max_depth=5, min_samples_leaf=5, random_state=1)\n",
      "Mean accuracy: 0.7895\n",
      "TP: 796, FN: 217\n",
      "FP: 204, TN: 783\n",
      "RandomForestClassifier(max_depth=7, min_samples_leaf=5, n_estimators=200,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.811\n",
      "TP: 819, FN: 197\n",
      "FP: 181, TN: 803\n",
      "RandomForestClassifier(max_depth=9, min_samples_leaf=5, n_estimators=200,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.825\n",
      "TP: 834, FN: 184\n",
      "FP: 166, TN: 816\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=5, n_estimators=300,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.826\n",
      "TP: 832, FN: 180\n",
      "FP: 168, TN: 820\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=10, n_estimators=300,\n",
      "                       random_state=2)\n",
      "Mean accuracy: 0.8225\n",
      "TP: 830, FN: 185\n",
      "FP: 170, TN: 815\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=15, n_estimators=300,\n",
      "                       random_state=2)\n",
      "Mean accuracy: 0.823\n",
      "TP: 828, FN: 182\n",
      "FP: 172, TN: 818\n",
      "RandomForestClassifier(max_depth=16, n_estimators=400, random_state=2)\n",
      "Mean accuracy: 0.8285\n",
      "TP: 840, FN: 183\n",
      "FP: 160, TN: 817\n",
      "RandomForestClassifier(max_depth=20, n_estimators=600, random_state=1)\n",
      "Mean accuracy: 0.8375\n",
      "TP: 846, FN: 171\n",
      "FP: 154, TN: 829\n",
      "RandomForestClassifier(max_depth=20, n_estimators=800, random_state=1)\n",
      "Mean accuracy: 0.832\n",
      "TP: 837, FN: 173\n",
      "FP: 163, TN: 827\n"
     ]
    }
   ],
   "source": [
    "try_random_forests('d2vdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da45bb8f-d1bb-4a78-a3ea-3f3fd60e5b22",
   "metadata": {},
   "source": [
    "## Random Forest + Doc2Vec (DBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5eea5e82-2749-4fc6-ac79-87cee9e2d5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=5, random_state=1)\n",
      "Mean accuracy: 0.8025\n",
      "TP: 833, FN: 228\n",
      "FP: 167, TN: 772\n",
      "RandomForestClassifier(max_depth=5, min_samples_leaf=5, random_state=1)\n",
      "Mean accuracy: 0.8005\n",
      "TP: 834, FN: 233\n",
      "FP: 166, TN: 767\n",
      "RandomForestClassifier(max_depth=7, min_samples_leaf=5, n_estimators=200,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.819\n",
      "TP: 857, FN: 219\n",
      "FP: 143, TN: 781\n",
      "RandomForestClassifier(max_depth=9, min_samples_leaf=5, n_estimators=200,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.8215\n",
      "TP: 857, FN: 214\n",
      "FP: 143, TN: 786\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=5, n_estimators=300,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.834\n",
      "TP: 860, FN: 192\n",
      "FP: 140, TN: 808\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=10, n_estimators=300,\n",
      "                       random_state=2)\n",
      "Mean accuracy: 0.8385\n",
      "TP: 868, FN: 191\n",
      "FP: 132, TN: 809\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=15, n_estimators=300,\n",
      "                       random_state=2)\n",
      "Mean accuracy: 0.8365\n",
      "TP: 868, FN: 195\n",
      "FP: 132, TN: 805\n",
      "RandomForestClassifier(max_depth=16, n_estimators=400, random_state=2)\n",
      "Mean accuracy: 0.831\n",
      "TP: 852, FN: 190\n",
      "FP: 148, TN: 810\n",
      "RandomForestClassifier(max_depth=20, n_estimators=600, random_state=1)\n",
      "Mean accuracy: 0.834\n",
      "TP: 859, FN: 191\n",
      "FP: 141, TN: 809\n",
      "RandomForestClassifier(max_depth=20, n_estimators=800, random_state=1)\n",
      "Mean accuracy: 0.8365\n",
      "TP: 864, FN: 191\n",
      "FP: 136, TN: 809\n"
     ]
    }
   ],
   "source": [
    "try_random_forests('d2vdbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a1bf8-3f65-4fec-9e00-5c4b5491ac04",
   "metadata": {},
   "source": [
    "## Random Forest + Doc2Vec (DM, 100 dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "278f7fd6-ae4c-4510-b03c-ad55d9380c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=5, random_state=1)\n",
      "Mean accuracy: 0.767\n",
      "TP: 760, FN: 226\n",
      "FP: 240, TN: 774\n",
      "RandomForestClassifier(max_depth=5, min_samples_leaf=5, random_state=1)\n",
      "Mean accuracy: 0.7665\n",
      "TP: 765, FN: 232\n",
      "FP: 235, TN: 768\n",
      "RandomForestClassifier(max_depth=7, min_samples_leaf=5, n_estimators=200,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.786\n",
      "TP: 788, FN: 216\n",
      "FP: 212, TN: 784\n",
      "RandomForestClassifier(max_depth=9, min_samples_leaf=5, n_estimators=200,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.7965\n",
      "TP: 792, FN: 199\n",
      "FP: 208, TN: 801\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=5, n_estimators=300,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.807\n",
      "TP: 802, FN: 188\n",
      "FP: 198, TN: 812\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=10, n_estimators=300,\n",
      "                       random_state=2)\n",
      "Mean accuracy: 0.802\n",
      "TP: 801, FN: 197\n",
      "FP: 199, TN: 803\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=15, n_estimators=300,\n",
      "                       random_state=2)\n",
      "Mean accuracy: 0.796\n",
      "TP: 790, FN: 198\n",
      "FP: 210, TN: 802\n",
      "RandomForestClassifier(max_depth=16, n_estimators=400, random_state=2)\n",
      "Mean accuracy: 0.8015\n",
      "TP: 793, FN: 190\n",
      "FP: 207, TN: 810\n",
      "RandomForestClassifier(max_depth=20, n_estimators=600, random_state=1)\n",
      "Mean accuracy: 0.808\n",
      "TP: 797, FN: 181\n",
      "FP: 203, TN: 819\n",
      "RandomForestClassifier(max_depth=20, n_estimators=800, random_state=1)\n",
      "Mean accuracy: 0.809\n",
      "TP: 802, FN: 184\n",
      "FP: 198, TN: 816\n"
     ]
    }
   ],
   "source": [
    "try_random_forests('d2vdm100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29581bc-287f-4a52-af7a-b9b66ce791fb",
   "metadata": {},
   "source": [
    "## Random Forest + Doc2Vec (DBOW, 100 dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9eb4bfb-304d-4031-9e9d-468d0715a211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=5, random_state=1)\n",
      "Mean accuracy: 0.7765\n",
      "TP: 797, FN: 244\n",
      "FP: 203, TN: 756\n",
      "RandomForestClassifier(max_depth=5, min_samples_leaf=5, random_state=1)\n",
      "Mean accuracy: 0.781\n",
      "TP: 801, FN: 239\n",
      "FP: 199, TN: 761\n",
      "RandomForestClassifier(max_depth=7, min_samples_leaf=5, n_estimators=200,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.799\n",
      "TP: 814, FN: 216\n",
      "FP: 186, TN: 784\n",
      "RandomForestClassifier(max_depth=9, min_samples_leaf=5, n_estimators=200,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.8115\n",
      "TP: 828, FN: 205\n",
      "FP: 172, TN: 795\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=5, n_estimators=300,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.8235\n",
      "TP: 836, FN: 189\n",
      "FP: 164, TN: 811\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=10, n_estimators=300,\n",
      "                       random_state=2)\n",
      "Mean accuracy: 0.8155\n",
      "TP: 845, FN: 214\n",
      "FP: 155, TN: 786\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=15, n_estimators=300,\n",
      "                       random_state=2)\n",
      "Mean accuracy: 0.8195\n",
      "TP: 847, FN: 208\n",
      "FP: 153, TN: 792\n",
      "RandomForestClassifier(max_depth=16, n_estimators=400, random_state=2)\n",
      "Mean accuracy: 0.825\n",
      "TP: 835, FN: 185\n",
      "FP: 165, TN: 815\n",
      "RandomForestClassifier(max_depth=20, n_estimators=600, random_state=1)\n",
      "Mean accuracy: 0.8185\n",
      "TP: 835, FN: 198\n",
      "FP: 165, TN: 802\n",
      "RandomForestClassifier(max_depth=20, n_estimators=800, random_state=1)\n",
      "Mean accuracy: 0.822\n",
      "TP: 843, FN: 199\n",
      "FP: 157, TN: 801\n"
     ]
    }
   ],
   "source": [
    "try_random_forests('d2vdbow100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8a78b0-b45d-410f-8973-d13638c3d3a5",
   "metadata": {},
   "source": [
    "## Random Forest + Word2Vec (avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03b6e989-fc4f-4612-81d5-243e30c4d29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=5, random_state=1)\n",
      "Mean accuracy: 0.72\n",
      "TP: 730, FN: 290\n",
      "FP: 270, TN: 710\n",
      "RandomForestClassifier(max_depth=5, min_samples_leaf=5, random_state=1)\n",
      "Mean accuracy: 0.7185\n",
      "TP: 733, FN: 296\n",
      "FP: 267, TN: 704\n",
      "RandomForestClassifier(max_depth=7, min_samples_leaf=5, n_estimators=200,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.7315\n",
      "TP: 734, FN: 271\n",
      "FP: 266, TN: 729\n",
      "RandomForestClassifier(max_depth=9, min_samples_leaf=5, n_estimators=200,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.7375\n",
      "TP: 741, FN: 266\n",
      "FP: 259, TN: 734\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=5, n_estimators=300,\n",
      "                       random_state=1)\n",
      "Mean accuracy: 0.7445\n",
      "TP: 738, FN: 249\n",
      "FP: 262, TN: 751\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=10, n_estimators=300,\n",
      "                       random_state=2)\n",
      "Mean accuracy: 0.743\n",
      "TP: 743, FN: 257\n",
      "FP: 257, TN: 743\n",
      "RandomForestClassifier(max_depth=15, min_samples_leaf=15, n_estimators=300,\n",
      "                       random_state=2)\n",
      "Mean accuracy: 0.7395\n",
      "TP: 739, FN: 260\n",
      "FP: 261, TN: 740\n",
      "RandomForestClassifier(max_depth=16, n_estimators=400, random_state=2)\n",
      "Mean accuracy: 0.7485\n",
      "TP: 744, FN: 247\n",
      "FP: 256, TN: 753\n",
      "RandomForestClassifier(max_depth=20, n_estimators=600, random_state=1)\n",
      "Mean accuracy: 0.7435\n",
      "TP: 736, FN: 249\n",
      "FP: 264, TN: 751\n",
      "RandomForestClassifier(max_depth=20, n_estimators=800, random_state=1)\n",
      "Mean accuracy: 0.746\n",
      "TP: 738, FN: 246\n",
      "FP: 262, TN: 754\n"
     ]
    }
   ],
   "source": [
    "try_random_forests('w2vavg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b148b-8235-48b2-9e6e-1fa02f46f342",
   "metadata": {},
   "source": [
    "# SVM + tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "746f3dfe-8b4e-4a88-9ab6-9e4024bad780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_svms(model_name, max_iter=1000):\n",
    "    X_train = eval('X_train_' + model_name)\n",
    "    X_test = eval('X_test_' + model_name)\n",
    "    svms = [\n",
    "        SVC(kernel='linear', max_iter=max_iter, random_state=1),\n",
    "        SVC(kernel='rbf', max_iter=max_iter, random_state=1),\n",
    "        SVC(kernel='poly', degree=2, max_iter=max_iter, random_state=1),\n",
    "        SVC(kernel='poly', degree=3, max_iter=max_iter, random_state=1),\n",
    "        SVC(kernel='sigmoid', max_iter=max_iter, random_state=1),\n",
    "    ]\n",
    "    for rfc in svms:\n",
    "        print(rfc)\n",
    "        try_classifier(rfc, X_train, X_test, big_table=False, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70fa487a-b15c-4667-994d-aa6440779d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(kernel='linear', max_iter=1000, random_state=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yasht\\fakenews\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8475\n",
      "TP: 829, FN: 134\n",
      "FP: 171, TN: 866\n",
      "SVC(max_iter=1000, random_state=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yasht\\fakenews\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.849\n",
      "TP: 849, FN: 151\n",
      "FP: 151, TN: 849\n",
      "SVC(degree=2, kernel='poly', max_iter=1000, random_state=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yasht\\fakenews\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8325\n",
      "TP: 811, FN: 146\n",
      "FP: 189, TN: 854\n",
      "SVC(kernel='poly', max_iter=1000, random_state=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yasht\\fakenews\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7505\n",
      "TP: 611, FN: 110\n",
      "FP: 389, TN: 890\n",
      "SVC(kernel='sigmoid', max_iter=1000, random_state=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yasht\\fakenews\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.843\n",
      "TP: 822, FN: 136\n",
      "FP: 178, TN: 864\n"
     ]
    }
   ],
   "source": [
    "try_svms('tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0791f8-f42b-4261-9242-f9ba1c0df343",
   "metadata": {},
   "source": [
    "## SVM + Doc2Vec (DM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20a14a1f-d77f-43ed-966a-c342db83571a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(kernel='linear', random_state=1)\n",
      "Mean accuracy: 0.85\n",
      "TP: 886, FN: 186\n",
      "FP: 114, TN: 814\n",
      "SVC(random_state=1)\n",
      "Mean accuracy: 0.846\n",
      "TP: 866, FN: 174\n",
      "FP: 134, TN: 826\n",
      "SVC(degree=2, kernel='poly', random_state=1)\n",
      "Mean accuracy: 0.8375\n",
      "TP: 856, FN: 181\n",
      "FP: 144, TN: 819\n",
      "SVC(kernel='poly', random_state=1)\n",
      "Mean accuracy: 0.838\n",
      "TP: 877, FN: 201\n",
      "FP: 123, TN: 799\n",
      "SVC(kernel='sigmoid', random_state=1)\n",
      "Mean accuracy: 0.8435\n",
      "TP: 880, FN: 193\n",
      "FP: 120, TN: 807\n"
     ]
    }
   ],
   "source": [
    "try_svms('d2vdm', max_iter=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b05ff4f-bb58-49a4-924e-59631a9f08ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8185\n",
      "TP: 913, FN: 276\n",
      "FP: 87, TN: 724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.77      0.83      1189\n",
      "        True       0.72      0.89      0.80       811\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.82      0.83      0.82      2000\n",
      "weighted avg       0.84      0.82      0.82      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_d2vdm_classifier(SVC(kernel='poly', degree=4, max_iter=-1, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af4d02-7778-4169-a055-5764cae2ad24",
   "metadata": {},
   "source": [
    "## SVM + Doc2Vec (DBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe0552bd-7df8-4e59-bf5b-44f81de13813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(kernel='linear', random_state=1)\n",
      "Mean accuracy: 0.862\n",
      "TP: 863, FN: 139\n",
      "FP: 137, TN: 861\n",
      "SVC(random_state=1)\n",
      "Mean accuracy: 0.8635\n",
      "TP: 875, FN: 148\n",
      "FP: 125, TN: 852\n",
      "SVC(degree=2, kernel='poly', random_state=1)\n",
      "Mean accuracy: 0.863\n",
      "TP: 860, FN: 134\n",
      "FP: 140, TN: 866\n",
      "SVC(kernel='poly', random_state=1)\n",
      "Mean accuracy: 0.849\n",
      "TP: 904, FN: 206\n",
      "FP: 96, TN: 794\n",
      "SVC(kernel='sigmoid', random_state=1)\n",
      "Mean accuracy: 0.855\n",
      "TP: 867, FN: 157\n",
      "FP: 133, TN: 843\n"
     ]
    }
   ],
   "source": [
    "try_svms('d2vdbow', max_iter=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44d5a790-f3ec-474e-abee-a43f2b34ee16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7645\n",
      "TP: 952, FN: 423\n",
      "FP: 48, TN: 577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.69      0.80      1375\n",
      "        True       0.58      0.92      0.71       625\n",
      "\n",
      "    accuracy                           0.76      2000\n",
      "   macro avg       0.76      0.81      0.76      2000\n",
      "weighted avg       0.83      0.76      0.77      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_d2vdbow_classifier(SVC(kernel='poly', degree=4, max_iter=-1, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85149c5-756a-460b-9125-42e27707675d",
   "metadata": {},
   "source": [
    "## SVM + Doc2Vec (DM, 100 dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d24f0fc6-1402-48e8-9698-f5466a068eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(kernel='linear', random_state=1)\n",
      "Mean accuracy: 0.85\n",
      "TP: 886, FN: 186\n",
      "FP: 114, TN: 814\n",
      "SVC(random_state=1)\n",
      "Mean accuracy: 0.846\n",
      "TP: 866, FN: 174\n",
      "FP: 134, TN: 826\n",
      "SVC(degree=2, kernel='poly', random_state=1)\n",
      "Mean accuracy: 0.8375\n",
      "TP: 856, FN: 181\n",
      "FP: 144, TN: 819\n",
      "SVC(kernel='poly', random_state=1)\n",
      "Mean accuracy: 0.838\n",
      "TP: 877, FN: 201\n",
      "FP: 123, TN: 799\n",
      "SVC(kernel='sigmoid', random_state=1)\n",
      "Mean accuracy: 0.8435\n",
      "TP: 880, FN: 193\n",
      "FP: 120, TN: 807\n"
     ]
    }
   ],
   "source": [
    "try_svms('d2vdm', max_iter=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e5beb42-6960-4ad2-be4e-ce95119970d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.8185\n",
      "TP: 913, FN: 276\n",
      "FP: 87, TN: 724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      0.77      0.83      1189\n",
      "        True       0.72      0.89      0.80       811\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.82      0.83      0.82      2000\n",
      "weighted avg       0.84      0.82      0.82      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_d2vdm_classifier(SVC(kernel='poly', degree=4, max_iter=-1, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c22314b-e812-4ed3-be4a-815e9615bd36",
   "metadata": {},
   "source": [
    "## SVM + Doc2Vec (DBOW, 100 dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8624bbca-e169-4639-aca3-cde7e3939e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(kernel='linear', random_state=1)\n",
      "Mean accuracy: 0.862\n",
      "TP: 863, FN: 139\n",
      "FP: 137, TN: 861\n",
      "SVC(random_state=1)\n",
      "Mean accuracy: 0.8635\n",
      "TP: 875, FN: 148\n",
      "FP: 125, TN: 852\n",
      "SVC(degree=2, kernel='poly', random_state=1)\n",
      "Mean accuracy: 0.863\n",
      "TP: 860, FN: 134\n",
      "FP: 140, TN: 866\n",
      "SVC(kernel='poly', random_state=1)\n",
      "Mean accuracy: 0.849\n",
      "TP: 904, FN: 206\n",
      "FP: 96, TN: 794\n",
      "SVC(kernel='sigmoid', random_state=1)\n",
      "Mean accuracy: 0.855\n",
      "TP: 867, FN: 157\n",
      "FP: 133, TN: 843\n"
     ]
    }
   ],
   "source": [
    "try_svms('d2vdbow', max_iter=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b12a9562-a33f-48a5-91ec-c67b3d17246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.7645\n",
      "TP: 952, FN: 423\n",
      "FP: 48, TN: 577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.69      0.80      1375\n",
      "        True       0.58      0.92      0.71       625\n",
      "\n",
      "    accuracy                           0.76      2000\n",
      "   macro avg       0.76      0.81      0.76      2000\n",
      "weighted avg       0.83      0.76      0.77      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_d2vdbow_classifier(SVC(kernel='poly', degree=4, max_iter=-1, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9cb9ae-abb9-4b75-93f0-e51ac89f8cf3",
   "metadata": {},
   "source": [
    "## SVM + Word2Vec (avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba6c55f4-8466-4f32-bbe8-c5cc5d45f198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(kernel='linear', random_state=1)\n",
      "Mean accuracy: 0.7615\n",
      "TP: 766, FN: 243\n",
      "FP: 234, TN: 757\n",
      "SVC(random_state=1)\n",
      "Mean accuracy: 0.7505\n",
      "TP: 745, FN: 244\n",
      "FP: 255, TN: 756\n",
      "SVC(degree=2, kernel='poly', random_state=1)\n",
      "Mean accuracy: 0.751\n",
      "TP: 735, FN: 233\n",
      "FP: 265, TN: 767\n",
      "SVC(kernel='poly', random_state=1)\n",
      "Mean accuracy: 0.7545\n",
      "TP: 736, FN: 227\n",
      "FP: 264, TN: 773\n",
      "SVC(kernel='sigmoid', random_state=1)\n",
      "Mean accuracy: 0.627\n",
      "TP: 638, FN: 384\n",
      "FP: 362, TN: 616\n"
     ]
    }
   ],
   "source": [
    "try_svms('w2vavg', max_iter=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a40a66-05ac-4598-b66a-82fd51bf4f1e",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93ccbd7f-53ce-4506-9467-cdf8bb2e85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "def try_extra_trees(model_name):\n",
    "    X_train = eval('X_train_' + model_name)\n",
    "    X_test = eval('X_test_' + model_name)\n",
    "    classifiers = [\n",
    "        ExtraTreesClassifier(max_depth=5, n_estimators=100, random_state=1),\n",
    "        ExtraTreesClassifier(max_depth=10, n_estimators=100, random_state=1),\n",
    "        ExtraTreesClassifier(max_depth=15, n_estimators=100, random_state=1),\n",
    "        ExtraTreesClassifier(max_depth=5, n_estimators=200, random_state=1),\n",
    "        ExtraTreesClassifier(max_depth=5, n_estimators=500, random_state=1),\n",
    "        ExtraTreesClassifier(max_depth=10, n_estimators=200, random_state=1),\n",
    "        ExtraTreesClassifier(max_depth=15, n_estimators=400, random_state=1),\n",
    "        ExtraTreesClassifier(max_depth=15, n_estimators=500, random_state=1),\n",
    "        ExtraTreesClassifier(max_depth=20, n_estimators=800, random_state=1),\n",
    "        ExtraTreesClassifier(max_depth=25, n_estimators=1000, random_state=1),\n",
    "        ExtraTreesClassifier(max_depth=30, n_estimators=1500, random_state=1),\n",
    "    ]\n",
    "    for cls in classifiers:\n",
    "        print(cls)\n",
    "        try_classifier(cls, X_train, X_test, big_table=False, name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4aafd-d460-40cc-b95f-a1475362deeb",
   "metadata": {},
   "source": [
    "## Extra Trees + tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "20d6de4c-7c44-432e-a2a9-b6985d06c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=5, random_state=1)\n",
      "Mean accuracy: 0.7835\n",
      "TP: 735, FN: 168\n",
      "FP: 265, TN: 832\n",
      "ExtraTreesClassifier(max_depth=10, random_state=1)\n",
      "Mean accuracy: 0.811\n",
      "TP: 764, FN: 142\n",
      "FP: 236, TN: 858\n",
      "ExtraTreesClassifier(max_depth=15, random_state=1)\n",
      "Mean accuracy: 0.8215\n",
      "TP: 785, FN: 142\n",
      "FP: 215, TN: 858\n",
      "ExtraTreesClassifier(max_depth=5, n_estimators=200, random_state=1)\n",
      "Mean accuracy: 0.8125\n",
      "TP: 771, FN: 146\n",
      "FP: 229, TN: 854\n",
      "ExtraTreesClassifier(max_depth=5, n_estimators=500, random_state=1)\n",
      "Mean accuracy: 0.83\n",
      "TP: 792, FN: 132\n",
      "FP: 208, TN: 868\n",
      "ExtraTreesClassifier(max_depth=10, n_estimators=200, random_state=1)\n",
      "Mean accuracy: 0.829\n",
      "TP: 793, FN: 135\n",
      "FP: 207, TN: 865\n",
      "ExtraTreesClassifier(max_depth=15, n_estimators=400, random_state=1)\n",
      "Mean accuracy: 0.8495\n",
      "TP: 812, FN: 113\n",
      "FP: 188, TN: 887\n",
      "ExtraTreesClassifier(max_depth=15, n_estimators=500, random_state=1)\n",
      "Mean accuracy: 0.849\n",
      "TP: 811, FN: 113\n",
      "FP: 189, TN: 887\n",
      "ExtraTreesClassifier(max_depth=20, n_estimators=800, random_state=1)\n",
      "Mean accuracy: 0.8525\n",
      "TP: 820, FN: 115\n",
      "FP: 180, TN: 885\n",
      "ExtraTreesClassifier(max_depth=25, n_estimators=1000, random_state=1)\n",
      "Mean accuracy: 0.8525\n",
      "TP: 829, FN: 124\n",
      "FP: 171, TN: 876\n",
      "ExtraTreesClassifier(max_depth=30, n_estimators=1500, random_state=1)\n",
      "Mean accuracy: 0.859\n",
      "TP: 838, FN: 120\n",
      "FP: 162, TN: 880\n"
     ]
    }
   ],
   "source": [
    "try_extra_trees('tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652312c3-171d-4c6d-a549-914cd6d3d026",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extra Trees + Doc2Vec (DM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a822e960-026a-4054-ad96-fd8053d7dcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=5, random_state=1)\n",
      "Mean accuracy: 0.8105\n",
      "TP: 837, FN: 216\n",
      "FP: 163, TN: 784\n",
      "ExtraTreesClassifier(max_depth=10, random_state=1)\n",
      "Mean accuracy: 0.8265\n",
      "TP: 851, FN: 198\n",
      "FP: 149, TN: 802\n",
      "ExtraTreesClassifier(max_depth=15, random_state=1)\n",
      "Mean accuracy: 0.8205\n",
      "TP: 844, FN: 203\n",
      "FP: 156, TN: 797\n",
      "ExtraTreesClassifier(max_depth=5, n_estimators=200, random_state=1)\n",
      "Mean accuracy: 0.826\n",
      "TP: 844, FN: 192\n",
      "FP: 156, TN: 808\n",
      "ExtraTreesClassifier(max_depth=5, n_estimators=500, random_state=1)\n",
      "Mean accuracy: 0.833\n",
      "TP: 862, FN: 196\n",
      "FP: 138, TN: 804\n",
      "ExtraTreesClassifier(max_depth=10, n_estimators=200, random_state=1)\n",
      "Mean accuracy: 0.831\n",
      "TP: 853, FN: 191\n",
      "FP: 147, TN: 809\n",
      "ExtraTreesClassifier(max_depth=15, n_estimators=400, random_state=1)\n",
      "Mean accuracy: 0.8385\n",
      "TP: 858, FN: 181\n",
      "FP: 142, TN: 819\n",
      "ExtraTreesClassifier(max_depth=15, n_estimators=500, random_state=1)\n",
      "Mean accuracy: 0.841\n",
      "TP: 859, FN: 177\n",
      "FP: 141, TN: 823\n",
      "ExtraTreesClassifier(max_depth=20, n_estimators=800, random_state=1)\n",
      "Mean accuracy: 0.843\n",
      "TP: 859, FN: 173\n",
      "FP: 141, TN: 827\n",
      "ExtraTreesClassifier(max_depth=25, n_estimators=1000, random_state=1)\n",
      "Mean accuracy: 0.843\n",
      "TP: 861, FN: 175\n",
      "FP: 139, TN: 825\n",
      "ExtraTreesClassifier(max_depth=30, n_estimators=1500, random_state=1)\n",
      "Mean accuracy: 0.8435\n",
      "TP: 861, FN: 174\n",
      "FP: 139, TN: 826\n"
     ]
    }
   ],
   "source": [
    "try_extra_trees('d2vdm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ccf43b-3959-40a1-8b6f-9c8249661065",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extra Trees + Doc2Vec (DBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "357f1ea1-3d4a-412e-8734-902a16ad5a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=5, random_state=1)\n",
      "Mean accuracy: 0.8135\n",
      "TP: 849, FN: 222\n",
      "FP: 151, TN: 778\n",
      "ExtraTreesClassifier(max_depth=10, random_state=1)\n",
      "Mean accuracy: 0.83\n",
      "TP: 865, FN: 205\n",
      "FP: 135, TN: 795\n",
      "ExtraTreesClassifier(max_depth=15, random_state=1)\n",
      "Mean accuracy: 0.8305\n",
      "TP: 854, FN: 193\n",
      "FP: 146, TN: 807\n",
      "ExtraTreesClassifier(max_depth=5, n_estimators=200, random_state=1)\n",
      "Mean accuracy: 0.8205\n",
      "TP: 861, FN: 220\n",
      "FP: 139, TN: 780\n",
      "ExtraTreesClassifier(max_depth=5, n_estimators=500, random_state=1)\n",
      "Mean accuracy: 0.8285\n",
      "TP: 873, FN: 216\n",
      "FP: 127, TN: 784\n",
      "ExtraTreesClassifier(max_depth=10, n_estimators=200, random_state=1)\n",
      "Mean accuracy: 0.8395\n",
      "TP: 871, FN: 192\n",
      "FP: 129, TN: 808\n",
      "ExtraTreesClassifier(max_depth=15, n_estimators=400, random_state=1)\n",
      "Mean accuracy: 0.8375\n",
      "TP: 864, FN: 189\n",
      "FP: 136, TN: 811\n",
      "ExtraTreesClassifier(max_depth=15, n_estimators=500, random_state=1)\n",
      "Mean accuracy: 0.838\n",
      "TP: 869, FN: 193\n",
      "FP: 131, TN: 807\n",
      "ExtraTreesClassifier(max_depth=20, n_estimators=800, random_state=1)\n",
      "Mean accuracy: 0.8415\n",
      "TP: 874, FN: 191\n",
      "FP: 126, TN: 809\n",
      "ExtraTreesClassifier(max_depth=25, n_estimators=1000, random_state=1)\n",
      "Mean accuracy: 0.8435\n",
      "TP: 884, FN: 197\n",
      "FP: 116, TN: 803\n",
      "ExtraTreesClassifier(max_depth=30, n_estimators=1500, random_state=1)\n",
      "Mean accuracy: 0.8425\n",
      "TP: 880, FN: 195\n",
      "FP: 120, TN: 805\n"
     ]
    }
   ],
   "source": [
    "try_extra_trees('d2vdbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d9b753-1fb9-4e98-8aff-40cec09fe8f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extra Trees + Doc2Vec (DM, 100 dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44098987-ceb9-427a-9280-fe8ada19ff43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=5, random_state=1)\n",
      "Mean accuracy: 0.802\n",
      "TP: 805, FN: 201\n",
      "FP: 195, TN: 799\n",
      "ExtraTreesClassifier(max_depth=10, random_state=1)\n",
      "Mean accuracy: 0.8035\n",
      "TP: 802, FN: 195\n",
      "FP: 198, TN: 805\n",
      "ExtraTreesClassifier(max_depth=15, random_state=1)\n",
      "Mean accuracy: 0.7985\n",
      "TP: 799, FN: 202\n",
      "FP: 201, TN: 798\n",
      "ExtraTreesClassifier(max_depth=5, n_estimators=200, random_state=1)\n",
      "Mean accuracy: 0.808\n",
      "TP: 803, FN: 187\n",
      "FP: 197, TN: 813\n",
      "ExtraTreesClassifier(max_depth=5, n_estimators=500, random_state=1)\n",
      "Mean accuracy: 0.8095\n",
      "TP: 807, FN: 188\n",
      "FP: 193, TN: 812\n",
      "ExtraTreesClassifier(max_depth=10, n_estimators=200, random_state=1)\n",
      "Mean accuracy: 0.8165\n",
      "TP: 805, FN: 172\n",
      "FP: 195, TN: 828\n",
      "ExtraTreesClassifier(max_depth=15, n_estimators=400, random_state=1)\n",
      "Mean accuracy: 0.821\n",
      "TP: 810, FN: 168\n",
      "FP: 190, TN: 832\n",
      "ExtraTreesClassifier(max_depth=15, n_estimators=500, random_state=1)\n",
      "Mean accuracy: 0.822\n",
      "TP: 811, FN: 167\n",
      "FP: 189, TN: 833\n",
      "ExtraTreesClassifier(max_depth=20, n_estimators=800, random_state=1)\n",
      "Mean accuracy: 0.8215\n",
      "TP: 815, FN: 172\n",
      "FP: 185, TN: 828\n",
      "ExtraTreesClassifier(max_depth=25, n_estimators=1000, random_state=1)\n",
      "Mean accuracy: 0.8285\n",
      "TP: 820, FN: 163\n",
      "FP: 180, TN: 837\n",
      "ExtraTreesClassifier(max_depth=30, n_estimators=1500, random_state=1)\n",
      "Mean accuracy: 0.83\n",
      "TP: 827, FN: 167\n",
      "FP: 173, TN: 833\n"
     ]
    }
   ],
   "source": [
    "try_extra_trees('d2vdm100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96155976-9387-4ca9-8430-a5e24c22cf55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extra Trees + Doc2Vec (DBOW, 100 dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b558ea8-082e-4aec-8275-ac8e4f6b4724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=5, random_state=1)\n",
      "Mean accuracy: 0.817\n",
      "TP: 819, FN: 185\n",
      "FP: 181, TN: 815\n",
      "ExtraTreesClassifier(max_depth=10, random_state=1)\n",
      "Mean accuracy: 0.828\n",
      "TP: 847, FN: 191\n",
      "FP: 153, TN: 809\n",
      "ExtraTreesClassifier(max_depth=15, random_state=1)\n",
      "Mean accuracy: 0.824\n",
      "TP: 861, FN: 213\n",
      "FP: 139, TN: 787\n",
      "ExtraTreesClassifier(max_depth=5, n_estimators=200, random_state=1)\n",
      "Mean accuracy: 0.81\n",
      "TP: 872, FN: 252\n",
      "FP: 128, TN: 748\n",
      "ExtraTreesClassifier(max_depth=5, n_estimators=500, random_state=1)\n",
      "Mean accuracy: 0.8165\n",
      "TP: 873, FN: 240\n",
      "FP: 127, TN: 760\n",
      "ExtraTreesClassifier(max_depth=10, n_estimators=200, random_state=1)\n",
      "Mean accuracy: 0.8305\n",
      "TP: 866, FN: 205\n",
      "FP: 134, TN: 795\n",
      "ExtraTreesClassifier(max_depth=15, n_estimators=400, random_state=1)\n",
      "Mean accuracy: 0.833\n",
      "TP: 857, FN: 191\n",
      "FP: 143, TN: 809\n",
      "ExtraTreesClassifier(max_depth=15, n_estimators=500, random_state=1)\n",
      "Mean accuracy: 0.834\n",
      "TP: 857, FN: 189\n",
      "FP: 143, TN: 811\n",
      "ExtraTreesClassifier(max_depth=20, n_estimators=800, random_state=1)\n",
      "Mean accuracy: 0.844\n",
      "TP: 873, FN: 185\n",
      "FP: 127, TN: 815\n",
      "ExtraTreesClassifier(max_depth=25, n_estimators=1000, random_state=1)\n",
      "Mean accuracy: 0.843\n",
      "TP: 860, FN: 174\n",
      "FP: 140, TN: 826\n",
      "ExtraTreesClassifier(max_depth=30, n_estimators=1500, random_state=1)\n",
      "Mean accuracy: 0.8405\n",
      "TP: 864, FN: 183\n",
      "FP: 136, TN: 817\n"
     ]
    }
   ],
   "source": [
    "try_extra_trees('d2vdbow100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c34fbd-c95f-41fa-9de9-54f8ecfca206",
   "metadata": {},
   "source": [
    "## Extra Trees + Word2Vec (avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03d70586-f9eb-48f3-8615-3387f95a6605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier(max_depth=5, random_state=1)\n",
      "Mean accuracy: 0.71\n",
      "TP: 735, FN: 315\n",
      "FP: 265, TN: 685\n",
      "ExtraTreesClassifier(max_depth=10, random_state=1)\n",
      "Mean accuracy: 0.7275\n",
      "TP: 741, FN: 286\n",
      "FP: 259, TN: 714\n",
      "ExtraTreesClassifier(max_depth=15, random_state=1)\n",
      "Mean accuracy: 0.7355\n",
      "TP: 737, FN: 266\n",
      "FP: 263, TN: 734\n",
      "ExtraTreesClassifier(max_depth=5, n_estimators=200, random_state=1)\n",
      "Mean accuracy: 0.7075\n",
      "TP: 738, FN: 323\n",
      "FP: 262, TN: 677\n",
      "ExtraTreesClassifier(max_depth=5, n_estimators=500, random_state=1)\n",
      "Mean accuracy: 0.7115\n",
      "TP: 738, FN: 315\n",
      "FP: 262, TN: 685\n",
      "ExtraTreesClassifier(max_depth=10, n_estimators=200, random_state=1)\n",
      "Mean accuracy: 0.7225\n",
      "TP: 735, FN: 290\n",
      "FP: 265, TN: 710\n",
      "ExtraTreesClassifier(max_depth=15, n_estimators=400, random_state=1)\n",
      "Mean accuracy: 0.7365\n",
      "TP: 738, FN: 265\n",
      "FP: 262, TN: 735\n",
      "ExtraTreesClassifier(max_depth=15, n_estimators=500, random_state=1)\n",
      "Mean accuracy: 0.737\n",
      "TP: 741, FN: 267\n",
      "FP: 259, TN: 733\n",
      "ExtraTreesClassifier(max_depth=20, n_estimators=800, random_state=1)\n",
      "Mean accuracy: 0.7445\n",
      "TP: 749, FN: 260\n",
      "FP: 251, TN: 740\n",
      "ExtraTreesClassifier(max_depth=25, n_estimators=1000, random_state=1)\n",
      "Mean accuracy: 0.7425\n",
      "TP: 738, FN: 253\n",
      "FP: 262, TN: 747\n",
      "ExtraTreesClassifier(max_depth=30, n_estimators=1500, random_state=1)\n",
      "Mean accuracy: 0.7435\n",
      "TP: 743, FN: 256\n",
      "FP: 257, TN: 744\n"
     ]
    }
   ],
   "source": [
    "try_extra_trees('w2vavg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1dee4-521b-490e-8c35-2121da70780f",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "102b4b53-8443-4c65-b679-b28658ac12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classifier_results.txt', 'w') as file:\n",
    "    for model_name, cls, acc, cm in classifier_results:\n",
    "        file.write(f\"\"\"{cls}\n",
    "Mean accuracy: {acc}\n",
    "TP: {cm[0][0]} | FN: {cm[0][1]}\n",
    "FP: {cm[1][0]} | TN: {cm[1][1]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431c2ce-a073-4d06-9d8c-1f7c4d5ec90c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
